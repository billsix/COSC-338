Notes as of 25 MAR

Homework 4 is due MON 30 MAR
Homework 5 is due WED 01 APR

Teleworking will be the default for the rest of the semester unless
you are told otherwise.  Currently, we are using Google Meet at normal
class times.  Invites will be sent over the class's slack channel.
We may move to Zoom.

Went over Chapter 5.  Ran ccube, triangle, jet, ambient, litjet,
shinyjet.  Discussed the implementation of each of them.
Showed the spot demo, but ran out of time to cover the implementation.  It's
the most difficult demo in the chapter, I recommend
reading it to try to figure out how it works, especially since
the light source is moving.

Discussed that I expect the final to be easier than
the midterm.  The final will be mainly based off of the
contents of the OpenGL Superbible book, as the homeworks
are.

There are 6 weeks remaining.  I updated the Syllabus to reflect
remaining material.

Towards each student's final
paper, I recommend finding something that you are interested
in studying on your own, and writing your paper about that.  I expect
3 to 5 pages minimum.  Anything you learn along the way is worth including,
even if it's not directly related to graphics (for instance, if you
need to figure out what malloc/free in C do to fully understand some
of the graphics code, spend the time to figure it out, and write about it)

It could be studying/modifying
the Minecraft implementation in Python, or the one in C. (Listed
on COSC's README.md)

It could be studying the spot demo from chapter 5 in detail, and porting
it to OpenGL 3.3 Core profile mode (shaders).  It could be changing
the movement in a project for some purpose.  Porting something
from C++ to Python and what you learned.  Porting something From GLUT to GLFW.
Explaining how pygments (in the Python Minecraft code) is different than GLFW.
Fix certain negative graphical artifacts which are due to aliasing.

Study how collision detection works in Minecraft.  Or in general, compare
and contrast how different games deal with collision detection.

Study raytracing.  Study a raytracer in 256 lines of C++
https://github.com/ssloy/tinyraytracer/wiki/Part-1:-understandable-raytracing


Study one or more of the projects from Code The Classics
https://github.com/Wireframe-Magazine/Code-the-Classics
(you can buy the book https://wireframe.raspberrypi.org/books/code-the-classics1)

Study http://www.opengl-tutorial.org/

Study https://learnopengl.com/Introduction.

Study modelloading.

Study how NDC could get rasterized in software, i.e., how are triangles
filled, etc?
https://github.com/ssloy/tinyrenderer

Study a McNopper Demo or two https://github.com/McNopper/OpenGL
(How to build it on Windows is on COSC-338's README.md)

Study some subsection of a large Program.  SuperTuxKart.
Neverball.

This is meant to be open ended and self directed.


Notes as of 08 MAR.

Mid term will be a take home exam, assigned on MON 09 MAR, due
FRI 13 MAR (Midterm.txt).  If you have questions, I suggest
reading through the comments in modelviewprojection's
code.

Class on 11 MAR is cancelled to allow more time for
students to take the exam.

Notes from 26 FEB

Midterm will 11 MAR 2020.  Review session will be 09 MAR 2020,
in class.

Rewent over strafing, which is moving in the x direction
instead of -1 units in the z.

Added project3. projects/project3Due04MAR2020.txt  Due a week from today
Added homework3.  homework/due04MAR2020.txt.  Due a week from today.  Busy week!
(These should not be difficult)

Compiled all demos from Chapter 3.  Ran them all and desribed
what was happening.  Did not get a chance to step through much
of the source code.  Spoke about differences between GLUT
and GLFW (callback based framework vs library in which the
programmer controls the event loop.  If missed class, the explanations
within the book are more than sufficienct.

In the book, because it does not have a moving camera in 3D space,
nor do objects move, the modelview and projection matrix are
set in the "resizewnidow" callback, and implicitly applied
to the modelspace data sent into glVertex et. al. in the
rendering callback.  (In my Python based demos, I always
set the modelview and projection matricies to be the identity
matrix so that this implied matrix multiplication was
essentially a no-op (no operation, i.e. the identity function)

A triangle has two faces, based on the ordering of the three vertices.
For a triangle, the front face is, not surpringly, defined by the right
hand rule, in opengl terms, it's "Counter clockwise".  The back face
is the other.  The ordering can be reconfigured, as the book demonstrates.

I like the right hand rule.  There will be questions on the test
that assume that you know what the right hand rule is and can
deduce things like front face, which way the zx plane rotates
when calling "rotate_y"


Notes from 24 FEB
Covered demo 18, which is about making a moving camera.
The position in x,y,z is calculated using the a vector in camera's
local coordinate system, which is converted to world space
using the view matrix.

After this calculation, the view matrix is returned to
it's originial state because the above operations were
wrapped in a with GLPushMatrix block.

Added mvpVisualization/demoPushMatrix.py to graphically
show how glpushmatrix/popmatrix is used to solve project 2.

Started the material from the OpenGL Blue Book, the superbible.
Briefly read through chapter 1 and 2.  Discussed GLUT, which
is like GLFW, but is more of a framework than a library, meaning
that GLUT controls the event loop, and the programmer needs
to a-priori specify the callbacks that should be called when
GLUT notices events.  This is in contrast to GLFW, where
the programmer retains control of the event loop.

Will soon start assigning reading from the BlueBook, along with assigning
homework questions based off of that reading.



Notes from 19 FEB

Covered same material as 17 FEB due to class absenses,
as this material is important to understand.

Notes from 17 FEB

Announced project 2, which is based off of material covered
today, demo19.
Covered demo17, which minimized code duplication
by means of a function stack.  Since functions
on the stack are last-in first-applied, we can
now read the modelspace to worldspace transformations
from the top of the code down to the bottom of the code.
The hardest part is still reading and understanding
the world space to cameraspace transformations.
Covered demo19, which uses openGL matricies in
the same manner as the function stack, but better
and more efficienct.  Covered glPushMatrix and glPopMatrix,
to which I've alluded in the demos where I have a set
of axises which are grayed out, but return to color
later.  Like Branch from Trolls.

Notes from 10 FEB

Annouced homework due 17 FEB, homework/due17FEB2020.txt
Showed demo14 again, to show that cameraspace is moved
to world space so that their -1 to 1 spaces match.
To explain again, the camera can be placed in a space
in the scene just like anything else, read the transformations
backwards.  But, since we need to take the world space
coordinates and turn them into camera space, we do
the inverse of the operations; by reversing their
order and negating the arguments.

Spoke about demo 15 again, and how the depth test
enables us to render the objects in any order, while
preserving the fact that objects nearer to the camera
will be visible, and will hide objects behind them.

Showed demo16, the first demo which actually works
like a normal first-person game.

Showed mvpVisualization/demoPerspective.py, my best demo,
which shows how the frustum can be turned into a rectangular
prism, which (via ortho), can be converted into NDC.  Anything
outside of NDC will be clipped out, and not drawn into the framebuffer,
hence, will never be turned into screen coordinates.

Quickly, and unthoroughly, discussed that demo17 removes
repetition in the code, by setting the perspective projection
once (instead of for all three objects, like in the previous demos),
sets the camera space once, and to place objects relative to
the world space, you just have to describe relative coordinates.

Covered the frustum, nearZ, farZ, top and left.  Discussed that
top will be half of 45 degrees, as our eyes see about 45 degress
vertically.  So what should left be?  Depends on the aspect
ratio of the monitor/framebuffer/viewport.  Some people may
have really wide monitors, and therefore, the front face
of the frustum should be much wider than it is tall.
This allows, say in games like FPS, for people with a wider
monitor to see more of the enemies.
In previous demos in 2D, to keep the paddles proportional,
we limited drawing to a subset of the screen that is always
square.
By making the front face of the frustum be proportional to
the monitor used, we keep proportionality of objects
in perspective mode, by scaling the front face to NDC,
which will then be converted to screen space appropriately.

Notes from 05 FEB 2020

Covered demo 14, 15, and showed 16 run but not the code.
Discussed Depth test, which is not enabled in 14, which means
that the blue square is always drawn, even though the z value
is further away.  Showed how demo15 solved it by enabling
and using the depth buffer, which acts similarly to the
color buffer, in that every frame of the event loop has
the value reset every frame to a default value.
Discussed that for the depth buffer, besides a default
value needing to be set per fragment, a test must also be specified to determine
if a fragment, needs to determin in the rendered geometry is
closer or further away than the existing geometry.
For instance, in the purple paddle is drawn first in the frame,
and then the blue square is drawn, but some of the pixels overlap,
depending on the value in the depth buffer for the exising purple
paddle, and depending on the depth value of the blue square, and
depending on the test set, the blue pixel will either be set
and the color of the fragment will be updated, or the blue color
will be discarded and the purple color and depth value (in NDC)
will remain.

Discussed how view space transformations are the least intuitive
aspect of graphics programming, because of the two viewpoints
(when you are driving, is the earth and everything on earth spinning,
or is the earth staying still and your car is moving?)
I recommend, and have extensive notes in demo14, on thinking
about the camera just like any other objects, by a series
of transformations to the local coordinate system.  But, in order
to conver the geometry of an object from world space to camera space,
you must invert the transformation sequence from camera space to world
space, thus creating a transformation sequence from world space
to camera space.

Showed mvpVisualization/demoViewWorldTopLevel.py to demostrate
that the camera moves to it's position (without moving the geometry),
but then inverts all of the transformations, this time bringing
the whole scene with it.

Demo 14 and 15 were the first (IMHO) poorly designed demos,
as they covered too much.  Each demo should fix 1 to 2 things from
the previous demo, and this one was too ambitious.  I need
to fix it to make it easy to read on one's own time.

Notes from 03 FEB 2020

Covered 2D rotations again, this time from a calc1 point of view.
For reference, https://www.alanzucconi.com/2016/02/03/2d-rotations/
Showed on the board how to put those into matrix form, and how
to view the columns of that matrix as the points where the x axis (1,0)
and y axis (0 1), go.  I will need to go over this more later, as
matrix representations of 2D, and 3D, rotations will be on exams,
even though day to day graphics programmming does not require
knowledge of the representation.

Covered demo 11 - 14, from the camera space demo up to
introducing a z coordinate.  Show how in Demo 13, if you reverse
the order of rendering, and draw the blue square first, then
the blue square will always be hidden behind the purple paddle.
And likewise in reverse.  This is leading up to discussing the
z buffer and how it allows for order-independent rendering of
the polygons, while retaining the fact that near objects
obscure far objects.

Covered the right hand rule, and used that as a basis for
showing what rotating around the x axis means, rotating
around the y axis means, and rotating around the z axis means.
The ability to create a matrix for rot_x, rot_y, and rot_z,
can easily be deduced from first principles, and that is
by knowing the right hand rule, describing where the
first component goes [1       [cos(theta)
                      0]  ->   sin(theta)]
second component goes [0        [-sin(theta)
                       1]  ->    cos(theta)]

resulting in matrix
  [cos(theta), -sin(theta)
   sin(theta), cos(theta)]

From the right and rule and this concept of rotating on a
plane (yz, zx, xy), the 3D rotation matrices are easy to deduce.

Discussed that from modelspace to world space, the method-chained
method calls should be read backwards, by envisioning a moving coordinate
system (origin, plus x and y axis), that translates and rotates.

Covered Python keyword arguments, as a way of seeing the argument's to
a called function at the function-call site.

Showed demos modelviewprojection/mvpVisualization/demo.py
modelviewprojection/mvpVisualization/demoAnimation.py
modelviewprojection/mvpVisualization/demoViewWorldTopLevel.py
to demonstrate what's happening.  Too many books show the transformations
abstractly from one space to another, without seeing the geometry transformed.
The Modelviewprojection codebase is meant to rectify that, while using
the opengl superbible v4 (which is an excellent book), to cover the rest
(colors, lighting, shadows, selection, textures, VAOs/VBOs/shaders.

I got feedback that given the paddle/square demos, that the concepts of
what I am doing with the method chaining are starting to click.
This is good, because although the codebase doesn't do anything fancy,
and it doesn't look good, it covers everything that students need to
know about relative objects, world space, and camera space.  And too
much pretty pictures and complex geometry would obfuscate the concept.

Notes from 29 JAN 2020
Covered from demo 6 - 10, camera management.
Covered making a directed acyclic graph on
the transformations, where NDC is always
relative to camera space, but showed how camera
space can be defined relative to world space,
or world space can be defined relative to camera
space. (When you're driving in your car, is your
car rotating the entire globe around while your
car is staying still, or is the earth still
and your car moving?)
Told class about written homework due Monday.


Notes from 27 JAN 2020

Assigned homework due Monday, a week from today.
Discussed project one, making a pong game in NDC,
but have not set a due date, as I have not yet implemented
it myself, which I will do before assigning any projects.
Told everyone about getting modelviewprojection up
and running on their home machines, as it is ungraded homework,
but people need to be able to run these programs on their own machines,
or in the lab, which has the requisite software (Visual Studio 2019
with Python Extension.)
Multiple people joined the slack channel for group
communication.
Discussed method chaining, how it works.  minimizes
naming one-time use variables.  Method chaining is used
for a simple version function composition, to introduce
the concept which later is turned into the lambda stack, and
then the matrix stacks.
Showed "Minecraft" in python, Craft in C, and McNabber's
OpenGL demos, to show what type of stuff is able to be done
with OpenGL.  Any of these projects can be a basis for their
final paper, which involves studying open source graphics code,
modifying it, or just doing a write-up of what they independenly
learned).
Showed the mvpvisualtion demos to show the endgame of where
modelviewprojection project is going, in order to motivate
while going through demos, towards adding motivation towards
understanding the intro material.
Demos the 3 scripts which try to rotate, but glossed over
the details due to time.  Go into the details of the math.
Did not cover demo05 onwards through camera management as I thought
I would, but that's ok, because the demos and history of graphics
(Crash Bandicoot vs Mario 64, the z-buffer, and the moving cameras)
was worthwhile.

Notes from 22 JAN 2020
 -Covered up to Demo06.  Update lesson 6 regarding Global Space (as I didn't explain what that meant, I just covered NDC)
 -Redo Demo6.  Repetition is good but slow down.  Don't let speed through the repetitive parts make me go to fast on the new parts
