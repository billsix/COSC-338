Notes from 10 FEB

Annouced homework due 17 FEB, homework/due17FEB2020.txt
Showed demo14 again, to show that cameraspace is moved
to world space so that their -1 to 1 spaces match.
To explain again, the camera can be placed in a space
in the scene just like anything else, read the transformations
backwards.  But, since we need to take the world space
coordinates and turn them into camera space, we do
the inverse of the operations; by reversing their
order and negating the arguments.

Spoke about demo 15 again, and how the depth test
enables us to render the objects in any order, while
preserving the fact that objects nearer to the camera
will be visible, and will hide objects behind them.

Showed demo16, the first demo which actually works
like a normal first-person game.

Showed mvpVisualization/demoPerspective.py, my best demo,
which shows how the frustum can be turned into a rectangular
prism, which (via ortho), can be converted into NDC.  Anything
outside of NDC will be clipped out, and not drawn into the framebuffer,
hence, will never be turned into screen coordinates.

Quickly, and unthoroughly, discussed that demo17 removes
repetition in the code, by setting the perspective projection
once (instead of for all three objects, like in the previous demos),
sets the camera space once, and to place objects relative to
the world space, you just have to describe relative coordinates.

Notes from 05 FEB 2020

Covered demo 14, 15, and showed 16 run but not the code.
Discussed Depth test, which is not enabled in 14, which means
that the blue square is always drawn, even though the z value
is further away.  Showed how demo15 solved it by enabling
and using the depth buffer, which acts similarly to the
color buffer, in that every frame of the event loop has
the value reset every frame to a default value.
Discussed that for the depth buffer, besides a default
value needing to be set per fragment, a test must also be specified to determine
if a fragment, needs to determin in the rendered geometry is
closer or further away than the existing geometry.
For instance, in the purple paddle is drawn first in the frame,
and then the blue square is drawn, but some of the pixels overlap,
depending on the value in the depth buffer for the exising purple
paddle, and depending on the depth value of the blue square, and
depending on the test set, the blue pixel will either be set
and the color of the fragment will be updated, or the blue color
will be discarded and the purple color and depth value (in NDC)
will remain.

Discussed how view space transformations are the least intuitive
aspect of graphics programming, because of the two viewpoints
(when you are driving, is the earth and everything on earth spinning,
or is the earth staying still and your car is moving?)
I recommend, and have extensive notes in demo14, on thinking
about the camera just like any other objects, by a series
of transformations to the local coordinate system.  But, in order
to conver the geometry of an object from world space to camera space,
you must invert the transformation sequence from camera space to world
space, thus creating a transformation sequence from world space
to camera space.

Showed mvpVisualization/demoViewWorldTopLevel.py to demostrate
that the camera moves to it's position (without moving the geometry),
but then inverts all of the transformations, this time bringing
the whole scene with it.

Demo 14 and 15 were the first (IMHO) poorly designed demos,
as they covered too much.  Each demo should fix 1 to 2 things from
the previous demo, and this one was too ambitious.  I need
to fix it to make it easy to read on one's own time.

Notes from 03 FEB 2020

Covered 2D rotations again, this time from a calc1 point of view.
For reference, https://www.alanzucconi.com/2016/02/03/2d-rotations/
Showed on the board how to put those into matrix form, and how
to view the columns of that matrix as the points where the x axis (1,0)
and y axis (0 1), go.  I will need to go over this more later, as
matrix representations of 2D, and 3D, rotations will be on exams,
even though day to day graphics programmming does not require
knowledge of the representation.

Covered demo 11 - 14, from the camera space demo up to
introducing a z coordinate.  Show how in Demo 13, if you reverse
the order of rendering, and draw the blue square first, then
the blue square will always be hidden behind the purple paddle.
And likewise in reverse.  This is leading up to discussing the
z buffer and how it allows for order-independent rendering of
the polygons, while retaining the fact that near objects
obscure far objects.

Covered the right hand rule, and used that as a basis for
showing what rotating around the x axis means, rotating
around the y axis means, and rotating around the z axis means.
The ability to create a matrix for rot_x, rot_y, and rot_z,
can easily be deduced from first principles, and that is
by knowing the right hand rule, describing where the
first component goes [1       [cos(theta)
                      0]  ->   sin(theta)]
second component goes [0        [-sin(theta)
                       1]  ->    cos(theta)]

resulting in matrix
  [cos(theta), -sin(theta)
   sin(theta), cos(theta)]

From the right and rule and this concept of rotating on a
plane (yz, zx, xy), the 3D rotation matrices are easy to deduce.

Discussed that from modelspace to world space, the method-chained
method calls should be read backwards, by envisioning a moving coordinate
system (origin, plus x and y axis), that translates and rotates.

Covered Python keyword arguments, as a way of seeing the argument's to
a called function at the function-call site.

Showed demos modelviewprojection/mvpVisualization/demo.py
modelviewprojection/mvpVisualization/demoAnimation.py
modelviewprojection/mvpVisualization/demoViewWorldTopLevel.py
to demonstrate what's happening.  Too many books show the transformations
abstractly from one space to another, without seeing the geometry transformed.
The Modelviewprojection codebase is meant to rectify that, while using
the opengl superbible v4 (which is an excellent book), to cover the rest
(colors, lighting, shadows, selection, textures, VAOs/VBOs/shaders.

I got feedback that given the paddle/square demos, that the concepts of
what I am doing with the method chaining are starting to click.
This is good, because although the codebase doesn't do anything fancy,
and it doesn't look good, it covers everything that students need to
know about relative objects, world space, and camera space.  And too
much pretty pictures and complex geometry would obfuscate the concept.

Notes from 29 JAN 2020
Covered from demo 6 - 10, camera management.
Covered making a directed acyclic graph on
the transformations, where NDC is always
relative to camera space, but showed how camera
space can be defined relative to world space,
or world space can be defined relative to camera
space. (When you're driving in your car, is your
car rotating the entire globe around while your
car is staying still, or is the earth still
and your car moving?)
Told class about written homework due Monday.


Notes from 27 JAN 2020

Assigned homework due Monday, a week from today.
Discussed project one, making a pong game in NDC,
but have not set a due date, as I have not yet implemented
it myself, which I will do before assigning any projects.
Told everyone about getting modelviewprojection up
and running on their home machines, as it is ungraded homework,
but people need to be able to run these programs on their own machines,
or in the lab, which has the requisite software (Visual Studio 2019
with Python Extension.)
Multiple people joined the slack channel for group
communication.
Discussed method chaining, how it works.  minimizes
naming one-time use variables.  Method chaining is used
for a simple version function composition, to introduce
the concept which later is turned into the lambda stack, and
then the matrix stacks.
Showed "Minecraft" in python, Craft in C, and McNabber's
OpenGL demos, to show what type of stuff is able to be done
with OpenGL.  Any of these projects can be a basis for their
final paper, which involves studying open source graphics code,
modifying it, or just doing a write-up of what they independenly
learned).
Showed the mvpvisualtion demos to show the endgame of where
modelviewprojection project is going, in order to motivate
while going through demos, towards adding motivation towards
understanding the intro material.
Demos the 3 scripts which try to rotate, but glossed over
the details due to time.  Go into the details of the math.
Did not cover demo05 onwards through camera management as I thought
I would, but that's ok, because the demos and history of graphics
(Crash Bandicoot vs Mario 64, the z-buffer, and the moving cameras)
was worthwhile.

Notes from 22 JAN 2020
 -Covered up to Demo06.  Update lesson 6 regarding Global Space (as I didn't explain what that meant, I just covered NDC)
 -Redo Demo6.  Repetition is good but slow down.  Don't let speed through the repetitive parts make me go to fast on the new parts
